{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fase = '05_lightgbm (un intento)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_config.json', 'r') as file:\n",
    "    gen_config =json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------COMIENZA----------------------------------------------\n",
      "--------------------------------------05_lightgbm (un intento)--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "folder = gen_config['folder']\n",
    "\n",
    "#entradas\n",
    "path_train = gen_config['path_train']\n",
    "path_test = gen_config['path_test']\n",
    "path_futuro = gen_config['path_futuro']\n",
    "path_prod_stats = gen_config['path_prod_stats']\n",
    "#path_transform_stats = gen_config['path_transform_stats']\n",
    "#salidas\n",
    "path_pred_test = gen_config['path_pred_test']\n",
    "path_pred_futuro = gen_config['path_pred_futuro']\n",
    "#variables\n",
    "lgbm_params = gen_config['var_lgbm_params']\n",
    "exclusiones = gen_config['var_exclusiones']\n",
    "dibujar_pesos = gen_config['var_dibujar_pesos']\n",
    "var_num_boost_round = gen_config['var_num_boost_round']\n",
    "clusters = gen_config['var_clusters']\n",
    "escalado = gen_config['var_escalado']\n",
    "with_mean = gen_config['var_withmean']\n",
    "\n",
    "print(f\"{'COMIENZA':-^100}\")\n",
    "print(f\"{fase:-^100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape df_train...........: (4767503, 177)\n",
      "Shape df_test............: (178684, 177)\n",
      "Shape df_futuro..........: (178684, 177)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_parquet(f\"{folder}/{path_train}\")\n",
    "df_test = pd.read_parquet(f\"{folder}/{path_test}\")\n",
    "df_futuro = pd.read_parquet(f\"{folder}/{path_futuro}\")\n",
    "\n",
    "prod_stats = pd.read_parquet(f\"{folder}/{path_prod_stats}\")\n",
    "#prod_stats = prod_stats[['product_id','customer_id', 'average_tn', 'std_dev_tn', 'total_tn', 'iqr_tn', 'median_tn']]\n",
    "#transform_stats = pd.read_parquet(f\"{folder}/{path_transform_stats}\")\n",
    "\n",
    "print(f\"{'Shape df_train':.<25}: {df_train.shape}\")\n",
    "print(f\"{'Shape df_test':.<25}: {df_test.shape}\")\n",
    "print(f\"{'Shape df_futuro':.<25}: {df_futuro.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos en tn_futuro de Test: 0\n",
      "Shape df_test dropna.....: (178684, 177)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nulos en tn_futuro de Test: {df_test['tn_futuro'].isna().sum()}\")\n",
    "#df_test['tn_futuro'] = df_test['tn_futuro'].fillna(0)\n",
    "df_test.dropna(subset=['tn_futuro'], inplace=True)\n",
    "print(f\"{'Shape df_test dropna':.<25}: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.9,\n",
      " 'bagging_freq': 1,\n",
      " 'boosting_type': 'gbdt',\n",
      " 'early_stopping_rounds': 10,\n",
      " 'feature_fraction': 0.9,\n",
      " 'force_col_wise': True,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_bin': 1023,\n",
      " 'max_depth': -1,\n",
      " 'metric': ['l2', 'rmse'],\n",
      " 'num_leaves': 70,\n",
      " 'num_threads': 16,\n",
      " 'objective': 'regression',\n",
      " 'verbose': 1,\n",
      " 'weight_column': 'mean_weight'}\n"
     ]
    }
   ],
   "source": [
    "pprint(lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRIBUCION DE DATOS EN CLUSTERS:\n",
      "           train  train_prop    test  test_prop  futuro  futuro_prop\n",
      "cluster                                                             \n",
      "0        3435423    0.720592  137333   0.768580  137333     0.768580\n",
      "1        1271780    0.266760   39485   0.220977   39485     0.220977\n",
      "2          60300    0.012648    1866   0.010443    1866     0.010443\n"
     ]
    }
   ],
   "source": [
    "distribution_report = pd.DataFrame(range(0,clusters[0]), columns=['cluster'])\n",
    "distribution_report['train'] = df_train[[f'cluster_dtw_{f\"{clusters[0]:02}\"}','periodo']].groupby(f'cluster_dtw_{f\"{clusters[0]:02}\"}').count()\n",
    "distribution_report['train_prop'] = distribution_report['train'] / distribution_report['train'].sum()\n",
    "distribution_report['test'] = df_test[[f'cluster_dtw_{f\"{clusters[0]:02}\"}','periodo']].groupby(f'cluster_dtw_{f\"{clusters[0]:02}\"}').count()\n",
    "distribution_report['test_prop'] = distribution_report['test'] / distribution_report['test'].sum()\n",
    "distribution_report['futuro'] = df_futuro[[f'cluster_dtw_{f\"{clusters[0]:02}\"}','periodo']].groupby(f'cluster_dtw_{f\"{clusters[0]:02}\"}').count()\n",
    "distribution_report['futuro_prop'] = distribution_report['futuro'] / distribution_report['futuro'].sum()\n",
    "distribution_report.set_index('cluster', inplace=True)\n",
    "\n",
    "print(f\"DISTRIBUCION DE DATOS EN CLUSTERS:\\n{distribution_report.head(clusters[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lgbm = df_train[f'cluster_dtw_{f\"{clusters[0]:02}\"}'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertidas a categorical: ['metodo_escalado', 'yearquarter', 'edad', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'presentacion']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = df_train.select_dtypes(['category']).columns.tolist()\n",
    "for col in categorical_features:\n",
    "    df_train[col] = df_train[col].cat.codes\n",
    "    df_test[col] = df_test[col].cat.codes\n",
    "    df_futuro[col] = df_futuro[col].cat.codes\n",
    "print(f\"Convertidas a categorical: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertidas a Boolean: ['max_2', 'max_3', 'max_4', 'max_5', 'max_6', 'max_7', 'max_8', 'max_9', 'max_10', 'max_11', 'max_12', 'max_13', 'max_15', 'max_18', 'crece_2', 'crece_3', 'crece_4', 'crece_5', 'crece_6', 'crece_7', 'crece_8', 'crece_9', 'crece_10', 'crece_11', 'crece_12', 'decrece_2', 'decrece_3', 'decrece_4', 'decrece_5', 'decrece_6', 'decrece_7', 'decrece_8', 'decrece_9', 'decrece_10', 'decrece_11', 'decrece_12']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = df_train.select_dtypes(['object']).columns.tolist()\n",
    "for col in categorical_features:\n",
    "    df_train[col] = df_train[col].astype('bool')\n",
    "    df_test[col] = df_test[col].astype('bool')\n",
    "    df_futuro[col] = df_futuro[col].astype('bool')\n",
    "print(f\"Convertidas a Boolean: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop datetime64: ['primer_periodo_prodcust', 'ultimo_periodo_prodcust']\n"
     ]
    }
   ],
   "source": [
    "date_features = df_train.select_dtypes(['datetime']).columns.tolist()\n",
    "\n",
    "df_train = df_train.drop(columns=date_features)\n",
    "df_test = df_test.drop(columns=date_features)\n",
    "df_futuro = df_futuro.drop(columns=date_features)\n",
    "print(f\"Drop datetime64: {date_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_cluster_ttf(df_train, df_test, df_futuro, cluster_col, cluster):\n",
    "    X_train = df_train[df_train[cluster_col] == cluster].iloc[:,:-1]\n",
    "    X_test = df_test[df_test[cluster_col] == cluster].iloc[:,:-1]\n",
    "    X_futuro = df_futuro[df_futuro[cluster_col] == cluster].iloc[:,:-1]\n",
    "\n",
    "    y_train = df_train[df_train[cluster_col] == cluster].iloc[:,-1]\n",
    "    y_test = df_test[df_test[cluster_col] == cluster].iloc[:,-1]\n",
    "    y_futuro = df_futuro[df_futuro[cluster_col] == cluster].iloc[:,-1]\n",
    "\n",
    "    print(f\"{'Cluster Column':.<25}: {cluster_col}\")\n",
    "    print(f\"{'Cluster':.<25}: {cluster}\")\n",
    "    print(f\"{'Shape X_train':.<25}: {X_train.shape}\")\n",
    "    print(f\"{'Shape X_test':.<25}: {X_test.shape}\")\n",
    "    print(f\"{'Shape X_futuro':.<25}: {X_futuro.shape}\")\n",
    "\n",
    "    print(f\"{'Shape y_train':.<25}: {y_train.shape}\")\n",
    "    print(f\"{'Shape y_test':.<25}: {y_test.shape}\")\n",
    "    print(f\"{'Shape y_futuro':.<25}: {y_futuro.shape}\")\n",
    "    print(f\"\\n\")\n",
    "\n",
    "    return X_train, X_test, X_futuro, y_train, y_test, y_futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster(X_train, X_test, X_futuro, y_train, y_test, y_futuro):\n",
    "\n",
    "    train_data = lgb.Dataset(X_train.drop(columns=exclusiones), label=y_train)\n",
    "    test_data = lgb.Dataset(X_test.drop(columns=exclusiones), label=y_test)\n",
    "    #futuro_data = lgb.Dataset(X_futuro.drop(columns=exclusiones), label=y_futuro)\n",
    "\n",
    "    params = lgbm_params\n",
    "\n",
    "    model = lgb.train(params,\n",
    "                    train_data,\n",
    "                    num_boost_round=var_num_boost_round,\n",
    "                    valid_sets=[test_data, train_data],\n",
    "                    )\n",
    "\n",
    "    y_pred = model.predict(X_test.drop(columns=exclusiones), num_iteration=model.best_iteration)\n",
    "    y_pred_futuro = model.predict(X_futuro.drop(columns=exclusiones), num_iteration=model.best_iteration)\n",
    "\n",
    "    return model, y_pred, y_pred_futuro\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Column...........: cluster_dtw_03\n",
      "Cluster..................: 1\n",
      "Shape X_train............: (1271780, 174)\n",
      "Shape X_test.............: (39485, 174)\n",
      "Shape X_futuro...........: (39485, 174)\n",
      "Shape y_train............: (1271780,)\n",
      "Shape y_test.............: (39485,)\n",
      "Shape y_futuro...........: (39485,)\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Total Bins 92587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1271780, number of used features: 159\n",
      "[LightGBM] [Info] Start training from score -0.002735\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[503]\ttraining's l2: 1.80836\ttraining's rmse: 1.34475\tvalid_0's l2: 2.04171\tvalid_0's rmse: 1.42888\n",
      "Cluster Column...........: cluster_dtw_03\n",
      "Cluster..................: 0\n",
      "Shape X_train............: (3435423, 174)\n",
      "Shape X_test.............: (137333, 174)\n",
      "Shape X_futuro...........: (137333, 174)\n",
      "Shape y_train............: (3435423,)\n",
      "Shape y_test.............: (137333,)\n",
      "Shape y_futuro...........: (137333,)\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Total Bins 98326\n",
      "[LightGBM] [Info] Number of data points in the train set: 3435423, number of used features: 159\n",
      "[LightGBM] [Info] Start training from score 0.005388\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's l2: 288.965\ttraining's rmse: 16.999\tvalid_0's l2: 3043.81\tvalid_0's rmse: 55.1707\n",
      "Cluster Column...........: cluster_dtw_03\n",
      "Cluster..................: 2\n",
      "Shape X_train............: (60300, 174)\n",
      "Shape X_test.............: (1866, 174)\n",
      "Shape X_futuro...........: (1866, 174)\n",
      "Shape y_train............: (60300,)\n",
      "Shape y_test.............: (1866,)\n",
      "Shape y_futuro...........: (1866,)\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Total Bins 90264\n",
      "[LightGBM] [Info] Number of data points in the train set: 60300, number of used features: 159\n",
      "[LightGBM] [Info] Start training from score 0.048239\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's l2: 662.249\ttraining's rmse: 25.7342\tvalid_0's l2: 1606.42\tvalid_0's rmse: 40.0802\n"
     ]
    }
   ],
   "source": [
    "modelos = []\n",
    "pred_final = pd.DataFrame()\n",
    "pred_final_futuro = pd.DataFrame()\n",
    "\n",
    "for cluster in clusters_lgbm:\n",
    "    X_train, X_test, X_futuro, y_train, y_test, y_futuro = separar_cluster_ttf(df_train, df_test, df_futuro, f'cluster_dtw_{f\"{clusters[0]:02}\"}', cluster)\n",
    "    model, y_pred, y_pred_futuro = train_cluster(X_train, X_test, X_futuro, y_train, y_test, y_futuro)\n",
    "\n",
    "    modelos.append(model)\n",
    "\n",
    "    pred = X_test[['periodo','product_id','customer_id','tn_norm']]\n",
    "    pred['cluster'] = cluster\n",
    "    pred['tn_futuro'] = y_test\n",
    "    pred['tn_prediccion'] = y_pred\n",
    "    pred_final = pd.concat([pred_final, pred], ignore_index=True, axis=0)\n",
    "\n",
    "    pred_futuro =X_futuro[['periodo','product_id','customer_id','tn_norm']]\n",
    "    pred_futuro['cluster'] = cluster\n",
    "    pred_futuro['tn_futuro'] = y_futuro\n",
    "    pred_futuro['tn_prediccion'] = y_pred_futuro\n",
    "    pred_final_futuro = pd.concat([pred_final_futuro, pred_futuro], ignore_index=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_inverse_transform(x_trans, lambda_, mean_, var_):\n",
    "    try:\n",
    "        x = x_trans.to_numpy().reshape(-1, 1)\n",
    "    except:\n",
    "        x = np.array(x_trans)\n",
    "\n",
    "    x = x * var_ ** 0.5 + mean_\n",
    "\n",
    "    x_inv = np.zeros_like(x)\n",
    "    pos = x >= 0\n",
    "\n",
    "    # when x >= 0\n",
    "    if abs(lambda_) < np.spacing(1.0):\n",
    "        x_inv[pos] = np.exp(x[pos]) - 1\n",
    "    else:  # lambda_ != 0\n",
    "        x_inv[pos] = np.power(x[pos] * lambda_ + 1, 1 / lambda_) - 1\n",
    "\n",
    "    # when x < 0\n",
    "    if abs(lambda_ - 2) > np.spacing(1.0):\n",
    "        x_inv[~pos] = 1 - np.power(-(2 - lambda_) * x[~pos] + 1, 1 / (2 - lambda_))\n",
    "    else:  # lambda_ == 2\n",
    "        x_inv[~pos] = 1 - np.exp(-x[~pos])\n",
    "\n",
    "    x_orig = x_inv.flatten()\n",
    "\n",
    "    return x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pred_final.merge(prod_stats, how='left', on=['product_id','customer_id'])\n",
    "#final = pred_final.merge(transform_stats, how='left', on=['product_id','customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse transform ROBUSTA SIN MEDIA\n"
     ]
    }
   ],
   "source": [
    "if escalado == 'tn_robust':\n",
    "    if with_mean:\n",
    "        final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['robust_scaler_scale'] + final['robust_scaler_center'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['robust_scaler_scale'] + final['robust_scaler_center']\n",
    "        print(\"inverse transform ROBUSTA CON MEDIA\")\n",
    "    else:\n",
    "        final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['robust_scaler_scale']# por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['robust_scaler_scale']\n",
    "        print(\"inverse transform ROBUSTA SIN MEDIA\")\n",
    "if escalado == 'tn_standard':\n",
    "    if with_mean:\n",
    "        final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['standard_scaler_scale'] + final['standard_scaler_mean'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['standard_scaler_scale'] + final['standard_scaler_mean']\n",
    "        print(\"inverse transform STANDARD CON MEDIA\")\n",
    "    else:\n",
    "        final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['standard_scaler_scale'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['standard_scaler_scale']\n",
    "        print(\"inverse transform STANDARD SIN MEDIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #power transform\n",
    "# final['tn_futuro_real'] = final.apply(lambda row: power_inverse_transform((row['tn_norm'] + row['tn_futuro']), row['pwr_lambda'], row['pwr_mean'], row['pwr_var']), axis=1)\n",
    "# final['tn_futuro_real'] = [tn_inv[0] for tn_inv in final['tn_futuro_real']]\n",
    "# final['tn_prediccion_real'] = final.apply(lambda row: power_inverse_transform((row['tn_norm'] + row['tn_prediccion']), row['pwr_lambda'], row['pwr_mean'], row['pwr_var']), axis=1)\n",
    "# final['tn_prediccion_real'] = [tn_inv[0] for tn_inv in final['tn_prediccion_real']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #robusta\n",
    "# final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['iqr_tn'] + final['median_tn'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "# final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['iqr_tn'] + final['median_tn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_futuro = pred_final_futuro.merge(prod_stats, how='left', on=['product_id','customer_id'])\n",
    "#final_futuro = pred_final_futuro.merge(transform_stats, how='left', on=['product_id','customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #power transform\n",
    "# final_futuro['tn_futuro_real'] = final_futuro.apply(lambda row: power_inverse_transform((row['tn_norm'] + row['tn_futuro']), row['pwr_lambda'], row['pwr_mean'], row['pwr_var']), axis=1)\n",
    "# final_futuro['tn_futuro_real'] = [tn_inv[0] for tn_inv in final_futuro['tn_futuro_real']]\n",
    "# final_futuro['tn_prediccion_real'] = final_futuro.apply(lambda row: power_inverse_transform((row['tn_norm'] + row['tn_prediccion']), row['pwr_lambda'], row['pwr_mean'], row['pwr_var']), axis=1)\n",
    "# final_futuro['tn_prediccion_real'] = [tn_inv[0] for tn_inv in final_futuro['tn_prediccion_real']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #robusta\n",
    "# final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['iqr_tn'] + final_futuro['median_tn'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "# final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['iqr_tn'] + final_futuro['median_tn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse transform ROBUSTA SIN MEDIA\n"
     ]
    }
   ],
   "source": [
    "if escalado == 'tn_robust':\n",
    "    if with_mean:\n",
    "        final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['robust_scaler_scale'] + final_futuro['robust_scaler_center'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['robust_scaler_scale'] + final_futuro['robust_scaler_center']\n",
    "        print(\"inverse transform ROBUSTA CON MEDIA\")\n",
    "    else:\n",
    "        final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['robust_scaler_scale']# por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['robust_scaler_scale']\n",
    "        print(\"inverse transform ROBUSTA SIN MEDIA\")\n",
    "if escalado == 'tn_standard':\n",
    "    if with_mean:\n",
    "        final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['standard_scaler_scale'] + final_futuro['standard_scaler_mean'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['standard_scaler_scale'] + final_futuro['standard_scaler_mean']\n",
    "        print(\"inverse transform STANDARD CON MEDIA\")\n",
    "    else:\n",
    "        final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['standard_scaler_scale'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "        final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['standard_scaler_scale']\n",
    "        print(\"inverse transform STANDARD SIN MEDIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = pred_final.merge(prod_stats, how='left', on=['product_id','customer_id'])\n",
    "# final['tn_futuro_real'] = (final['tn_norm'] + final['tn_futuro']) * final['std_dev_tn'] + final['average_tn'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "# final['tn_prediccion_real'] = (final['tn_norm'] + final['tn_prediccion']) * final['std_dev_tn'] + final['average_tn']\n",
    "final.to_parquet(f'{folder}/{path_pred_test}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_futuro = pred_final_futuro.merge(prod_stats, how='left', on=['product_id','customer_id'])\n",
    "# final_futuro['tn_futuro_real'] = (final_futuro['tn_norm'] + final_futuro['tn_futuro']) * final_futuro['std_dev_tn'] + final_futuro['average_tn'] # por dos porque esta normalizado y al hacer sumas y restas se acumulan medias\n",
    "# final_futuro['tn_prediccion_real'] = (final_futuro['tn_norm'] + final_futuro['tn_prediccion']) * final_futuro['std_dev_tn'] + final_futuro['average_tn']\n",
    "final_futuro.to_parquet(f'{folder}/{path_pred_futuro}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estado_control = f\"05_lightgbm Terminado - {nombrefile} - {datetime.now()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb.plot_importance(model, max_num_features=20, figsize=(10,10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_df = (\n",
    "#     pd.DataFrame({\n",
    "#         'feature_name': model.feature_name(),\n",
    "#         'importance_gain': model.feature_importance(importance_type='gain'),\n",
    "#         'importance_split': model.feature_importance(importance_type='split'),\n",
    "#     })\n",
    "#     .sort_values('importance_gain', ascending=False)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "# importance_df.sort_values('importance_split', ascending=False, inplace=True)\n",
    "# feat_dibujar = importance_df[0:20]['feature_name'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dibujar_pesos==True:\n",
    "    fig, axs = plt.subplots(5, 4, figsize=(20, 25))\n",
    "    d = -1\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            d+=1\n",
    "            lgb.plot_split_value_histogram(model,\n",
    "                            feature=feat_dibujar[d],\n",
    "                            bins=\"auto\",\n",
    "                            ax=axs[j, i]\n",
    "                            ,title=f\"Feat: {feat_dibujar[d]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------05_lightgbm (un intento)--------------------------------------\n",
      "----------------------------------------------FINALIZA----------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{fase:-^100}\")\n",
    "print(f\"{'FINALIZA':-^100}\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
