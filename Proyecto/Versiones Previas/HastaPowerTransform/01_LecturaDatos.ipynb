{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fase = '01_LecturaDatos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_config.json', 'r') as file:\n",
    "    gen_config =json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------COMIENZA----------------------------------------------\n",
      "------------------------------------------01_LecturaDatos-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "folder = gen_config['folder']\n",
    "\n",
    "#entradas\n",
    "\n",
    "#salidas\n",
    "path_group = gen_config['path_group']\n",
    "path_prod_stats = gen_config['path_prod_stats']\n",
    "path_prod_data = gen_config['path_prod_data']\n",
    "path_stock_data = gen_config['path_stock_data']\n",
    "path_overall_prod_stats = gen_config['path_overall_prod_stats']\n",
    "#variables\n",
    "\n",
    "print(f\"{'COMIENZA':-^100}\")\n",
    "print(f\"{fase:-^100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('../sell-in.txt', separator='\\t')\n",
    "prod_a_predecir = pl.read_csv('../productos_a_predecir.txt', separator='\\t')\n",
    "prod_data = pl.read_csv('../tb_productos_descripcion.txt', separator='\\t')\n",
    "stock_data = pl.read_csv('../tb_stocks.txt', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retenemos solo los productos en la lista de Productos A Predecir (los 780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sell-in Shape: (2_945_818,7, productos unicos: 1233)\n",
      "df Shape     : (2_293_481,7, productos unicos: 780)\n"
     ]
    }
   ],
   "source": [
    "print(f\"sell-in Shape: ({df.shape[0]:>9_d},{df.shape[1]:_d}, productos unicos: {df['product_id'].unique().count()})\")\n",
    "df = df.filter(pl.col('product_id').is_in(prod_a_predecir['product_id']))\n",
    "print(f\"df Shape     : ({df.shape[0]:>9_d},{df.shape[1]:_d}, productos unicos: {df['product_id'].unique().count()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataframe con agregacion a nivel producto-cliente\n",
    "\n",
    "en adelante usando \"prodcust\" para referinos a una combinacion de producto-cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_grouped_prodcust Shape: (2_293_481,4, productos unicos: 780)\n"
     ]
    }
   ],
   "source": [
    "df_grouped_prodcust = (\n",
    "    df.group_by(['periodo', 'product_id', 'customer_id'])\n",
    "    .agg(pl.col('tn').sum())\n",
    ")\n",
    "print(f\"df_grouped_prodcust Shape: ({df_grouped_prodcust.shape[0]:>9_d},{df_grouped_prodcust.shape[1]:_d}, productos unicos: {df_grouped_prodcust['product_id'].unique().count()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prod_stats va a retener datos estadisticos por prodcust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod_stats Shape:    (  262_805,6, productos unicos: 780)\n"
     ]
    }
   ],
   "source": [
    "prod_stats = df_grouped_prodcust.group_by(['product_id','customer_id']).agg(\n",
    "    [\n",
    "        pl.min('periodo').alias('primer_periodo'),\n",
    "        pl.max('periodo').alias('ultimo_periodo'),\n",
    "        pl.len().alias('values'),\n",
    "        pl.sum('tn').alias('total_tn'),\n",
    "    ]\n",
    ").sort(['product_id', 'customer_id'])\n",
    "\n",
    "prod_stats = prod_stats.with_columns(\n",
    "    (pl.col(\"primer_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\"),\n",
    "    (pl.col(\"ultimo_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\")\n",
    ")\n",
    "\n",
    "print(f\"prod_stats Shape:    ({prod_stats.shape[0]:>9_d},{prod_stats.shape[1]:_d}, productos unicos: {prod_stats['product_id'].unique().count()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora un dataframe similar pero con agrupacion a nivel producto\n",
    "\n",
    "de particular utilidad para el primer y ultimo periodo de cada producto, que se usa para rellenar ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_prod = (\n",
    "    df.group_by(['periodo', 'product_id'])\n",
    "    .agg(pl.col('tn').sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight stats con la intencion de pasarlo al parametro weight del modelo en las ultrimas etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_stats = df_grouped_prod.filter(pl.col('periodo') >= 201901).group_by(['product_id']).agg(\n",
    "    [\n",
    "        pl.sum('tn').alias('tot_weight'),\n",
    "        pl.mean('tn').alias('avg_weight'),\n",
    "        pl.median('tn').alias('med_weight'),\n",
    "    ]\n",
    ").sort(['product_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overall_prod_stats va a retener datos estadisticos por producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_prod_stats = df_grouped_prod.group_by(['product_id']).agg(\n",
    "    [\n",
    "        pl.min('periodo').alias('primer_periodo'),\n",
    "        pl.max('periodo').alias('ultimo_periodo'),\n",
    "        pl.len().alias('values'),\n",
    "        pl.sum('tn').alias('total_tn'),\n",
    "        pl.min('tn').alias('min_tn'),\n",
    "        pl.mean('tn').alias('average_tn'),\n",
    "        pl.median('tn').alias('median_tn'),\n",
    "        pl.std('tn',0).alias('std_dev_tn'),\n",
    "        (pl.col('tn').quantile(0.75)).alias('Q3_tn'),\n",
    "        (pl.col('tn').quantile(0.25)).alias('Q1_tn'),\n",
    "        pl.max('tn').alias('max_tn'),\n",
    "    ]\n",
    ").sort(['product_id'])\n",
    "\n",
    "overall_prod_stats = overall_prod_stats.with_columns(\n",
    "    [\n",
    "        (pl.col('Q3_tn') - pl.col('Q1_tn')).alias('iqr_tn'),\n",
    "    ]\n",
    ").sort(['product_id'])\n",
    "\n",
    "overall_prod_stats = overall_prod_stats.with_columns(\n",
    "    (pl.col(\"primer_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\"),\n",
    "    (pl.col(\"ultimo_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\")\n",
    ")\n",
    "\n",
    "overall_prod_stats = overall_prod_stats.join(weight_stats, on='product_id', how='left', coalesce=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combinaciones: todos los prodcust existentes en el datafram\n",
    "\n",
    "periodos: la lista completa de periodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaciones = df_grouped_prodcust.select([\"product_id\", \"customer_id\"]).unique().sort([\"product_id\",'customer_id'])\n",
    "todos_periodos = df_grouped_prodcust.select([\"periodo\"]).unique().sort(\"periodo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tomamos COMBINACIONES y con un cross join con PERIODOS se obtiene el universo de datos posibles para todas las series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = combinaciones.join(todos_periodos, how=\"cross\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con este primer join se obtiene el primer y ultimo periodo a nivel producto\n",
    "\n",
    "tambien agregamos \"periodo_dt\" en formato fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = df_completo.join(overall_prod_stats, on=[\"product_id\"], how=\"left\", coalesce=True).select([\"product_id\", \"customer_id\", \"periodo\", \"primer_periodo\", \"ultimo_periodo\"])\n",
    "df_completo = df_completo.with_columns([\n",
    "    ((pl.col(\"periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\")).alias(\"periodo_dt\"),  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "este filtro controla q la linea valide q el dato de periodo estÃ© dentro de la franja de existencia del producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = df_completo.filter((pl.col('periodo_dt') >= pl.col('primer_periodo')) & (pl.col('periodo_dt') <= pl.col('ultimo_periodo')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con esto se recupera el dato de tn para cada fila, como es un left join, los que tengan el dato, lo obtienen\n",
    "\n",
    "los que queden nulos, son periodos donde el producto ya existe pero ese cliente no hizo una compra, entonces se imputan a cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = df_completo.join(df_grouped_prodcust, on=['product_id', 'customer_id','periodo'], how='left', coalesce=True)\n",
    "df_completo = df_completo.with_columns(\n",
    "    pl.col(\"tn\").fill_null(0)\n",
    ")\n",
    "df_completo = df_completo.drop(['primer_periodo','ultimo_periodo','periodo_dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculamos la feature \"presentacion\" en bas eal sku_size\n",
    "- el maximo es grande\n",
    "- el minimo es chico\n",
    "- si maximo y minimo coinciden es unico\n",
    "- caso contrario es medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data_skuinfo = prod_data.select(['cat1', 'cat2', 'cat3', 'brand','descripcion','sku_size'])\n",
    "prod_data_skuinfo = prod_data_skuinfo.set_sorted(\"cat1\", \"cat2\", \"cat3\", \"brand\", \"descripcion\")\n",
    "prod_data_skuinfo = prod_data_skuinfo.group_by(['cat1', 'cat2', 'cat3', 'brand', 'descripcion']).agg([\n",
    "    pl.col(\"sku_size\").max().alias(\"max_skusize\"),\n",
    "    pl.col(\"sku_size\").min().alias(\"min_skusize\"),\n",
    "    ])\n",
    "prod_data = prod_data.join(prod_data_skuinfo, on=['cat1', 'cat2', 'cat3', 'brand', 'descripcion'], how='left', coalesce=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_expr = (\n",
    "    pl.when((pl.col('sku_size') == pl.col('max_skusize')) & (pl.col('sku_size') == pl.col('min_skusize')) )\n",
    "    .then(pl.lit(\"Unico\"))\n",
    "    .when((pl.col('sku_size') == pl.col('max_skusize')))\n",
    "    .then(pl.lit(\"Grande\"))\n",
    "    .when((pl.col('sku_size') == pl.col('min_skusize')))\n",
    "    .then(pl.lit(\"Chico\"))\n",
    "    .otherwise(pl.lit(\"Medio\"))\n",
    ")\n",
    "\n",
    "prod_data = prod_data.with_columns(conditional_expr.cast(pl.Categorical).alias('presentacion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_completo.filter((pl.col('product_id') == 20667) & (pl.col('customer_id') == 10427))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporte de datos a descartar\n",
    "\n",
    "prodcust que no hayan tenido ventas desde junio y que no tengan 12 datos en total\n",
    "\n",
    "se ve el porcentaje del total de toneladas q representan (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de datos a descartar\n",
      "shape: (12, 7)\n",
      "âââââââââââââ¬ââââââââââââââââ¬âââââââââââ¬ââââââââââââââââ¬ââââââââââââââââ¬âââââââââââââ¬âââââââââââââââ\n",
      "â Datos     â Total tn      â Cuenta   â Total tn (pre â Cuenta        â Total tn % â Total tn     â\n",
      "â presentes â ---           â ProdCust â junio)        â ProdCust (pre â ---        â (pre junio)  â\n",
      "â ---       â f64           â ---      â ---           â junio)        â f64        â %            â\n",
      "â i64       â               â i64      â f64           â ---           â            â ---          â\n",
      "â           â               â          â               â i64           â            â f64          â\n",
      "âââââââââââââªââââââââââââââââªâââââââââââªââââââââââââââââªââââââââââââââââªâââââââââââââªâââââââââââââââ¡\n",
      "â 1         â 2526.49537    â 44518    â 1245.37808    â 29204         â 0.002251   â 0.001109     â\n",
      "â 2         â 6815.40544    â 73667    â 3031.92855    â 45345         â 0.006071   â 0.002701     â\n",
      "â 3         â 12256.17277   â 96002    â 4985.70245    â 56280         â 0.010918   â 0.004441     â\n",
      "â 4         â 18716.67392   â 113958   â 7061.1083     â 63970         â 0.016673   â 0.00629      â\n",
      "â 5         â 26190.57573   â 128999   â 9080.58158    â 69567         â 0.02333    â 0.008089     â\n",
      "â 6         â 35614.53949   â 141761   â 11350.18402   â 73664         â 0.031725   â 0.010111     â\n",
      "â 7         â 46958.71092   â 153131   â 13606.07784   â 76716         â 0.04183    â 0.01212      â\n",
      "â 8         â 58304.08585   â 163328   â 16071.90824   â 79051         â 0.051936   â 0.014317     â\n",
      "â 9         â 70856.48511   â 172437   â 18360.87491   â 80816         â 0.063118   â 0.016356     â\n",
      "â 10        â 83443.45956   â 180764   â 20409.54511   â 82165         â 0.07433    â 0.018181     â\n",
      "â 11        â 97488.43017   â 188293   â 22557.3833    â 83260         â 0.086841   â 0.020094     â\n",
      "â 12        â 111217.29129  â 195368   â 24983.68334   â 84121         â 0.099071   â 0.022255     â\n",
      "âââââââââââââ´ââââââââââââââââ´âââââââââââ´ââââââââââââââââ´ââââââââââââââââ´âââââââââââââ´âââââââââââââââ\n"
     ]
    }
   ],
   "source": [
    "rep_tot = prod_stats['total_tn'].sum()\n",
    "rep_tot_tn = []\n",
    "rep_count = []\n",
    "rep_tot_pre = []\n",
    "rep_count_pre = []\n",
    "\n",
    "\n",
    "for i in range(12):\n",
    "    rep_tot_tn.append(      prod_stats.filter((pl.col('values') <= i+1))['total_tn'].sum())\n",
    "    rep_count.append(       prod_stats.filter((pl.col('values') <= i+1))['total_tn'].count())\n",
    "    rep_tot_pre.append(     prod_stats.filter((pl.col('values') <= i+1) & (pl.col('ultimo_periodo') < pl.date(2019,6,1)))['total_tn'].sum())\n",
    "    rep_count_pre.append(   prod_stats.filter((pl.col('values') <= i+1) & (pl.col('ultimo_periodo') < pl.date(2019,6,1)))['total_tn'].count())\n",
    "\n",
    "rep = pl.DataFrame({'Datos presentes': range(1, 13), 'Total tn': rep_tot_tn, 'Cuenta ProdCust': rep_count, 'Total tn (pre junio)': rep_tot_pre, 'Cuenta ProdCust (pre junio)': rep_count_pre})\n",
    "rep = rep.with_columns((pl.col('Total tn') / rep_tot).alias('Total tn %'))\n",
    "rep = rep.with_columns((pl.col('Total tn (pre junio)') / rep_tot).alias('Total tn (pre junio) %'))\n",
    "\n",
    "pl.Config.set_tbl_rows(20)\n",
    "print(\"Reporte de datos a descartar\")\n",
    "print(rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo las filas a descartar\n",
    "\n",
    "condition1 = pl.col('values') <= 12\n",
    "condition2 = pl.col('ultimo_periodo') < pl.date(2019,6,1)\n",
    "\n",
    "drop_prodcusts = prod_stats.filter((condition1 & condition2))[['product_id', 'customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en df_completo se descartaron 84_121 / 262_805 combinaciones de ProdCusts\n",
      "Shape df_completo: 5_303_555, 4, productos unicos: 780\n",
      "en prod_stats se descartaron 84_121 / 262_805 combinaciones de ProdCusts\n",
      "Shape prod_stats: 178_684, 6, productos unicos: 780\n"
     ]
    }
   ],
   "source": [
    "print(f\"en df_completo se descartaron {drop_prodcusts.shape[0]:_d} / {prod_stats.shape[0]:_d} combinaciones de ProdCusts\")\n",
    "df_completo = df_completo.join(drop_prodcusts, on=['product_id', 'customer_id'], how='anti', coalesce=True)\n",
    "print(f\"Shape df_completo: {df_completo.shape[0]:_d}, {df_completo.shape[1]:_d}, productos unicos: {df_completo['product_id'].unique().shape[0]}\")\n",
    "\n",
    "print(f\"en prod_stats se descartaron {drop_prodcusts.shape[0]:_d} / {prod_stats.shape[0]:_d} combinaciones de ProdCusts\")\n",
    "prod_stats = prod_stats.join(drop_prodcusts, on=['product_id', 'customer_id'], how='anti', coalesce=True)\n",
    "print(f\"Shape prod_stats: {prod_stats.shape[0]:_d}, {prod_stats.shape[1]:_d}, productos unicos: {prod_stats['product_id'].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod_stats Shape:    (  178_684,15, productos unicos: 780)\n"
     ]
    }
   ],
   "source": [
    "temp = df_completo.group_by(['product_id','customer_id']).agg(\n",
    "    [\n",
    "        pl.sum('tn').alias('total_tn'),\n",
    "        pl.min('tn').alias('min_tn'),\n",
    "        pl.mean('tn').alias('average_tn'),\n",
    "        pl.median('tn').alias('median_tn'),\n",
    "        pl.std('tn',0).alias('std_dev_tn'),\n",
    "        (pl.col('tn').quantile(0.75)).alias('Q3_tn'),\n",
    "        (pl.col('tn').quantile(0.25)).alias('Q1_tn'),\n",
    "        pl.max('tn').alias('max_tn')\n",
    "    ]\n",
    ")\n",
    "\n",
    "temp = temp.with_columns(\n",
    "    [\n",
    "        (pl.col('Q3_tn') - pl.col('Q1_tn')).alias('iqr_tn'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prod_stats = prod_stats.join(temp, on=['product_id', 'customer_id'], how='left', coalesce=True)\n",
    "\n",
    "print(f\"prod_stats Shape:    ({prod_stats.shape[0]:>9_d},{prod_stats.shape[1]:_d}, productos unicos: {prod_stats['product_id'].unique().count()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo serie con ceros\n",
      "shape: (10, 4)\n",
      "ââââââââââââââ¬ââââââââââââââ¬ââââââââââ¬ââââââââââ\n",
      "â product_id â customer_id â periodo â tn      â\n",
      "â ---        â ---         â ---     â ---     â\n",
      "â i64        â i64         â i64     â f64     â\n",
      "ââââââââââââââªââââââââââââââªââââââââââªââââââââââ¡\n",
      "â 21276      â 10462       â 201903  â 0.00075 â\n",
      "â 21276      â 10462       â 201904  â 0.0     â\n",
      "â 21276      â 10462       â 201905  â 0.00148 â\n",
      "â 21276      â 10462       â 201906  â 0.0     â\n",
      "â 21276      â 10462       â 201907  â 0.0     â\n",
      "â 21276      â 10462       â 201908  â 0.0     â\n",
      "â 21276      â 10462       â 201909  â 0.0     â\n",
      "â 21276      â 10462       â 201910  â 0.00075 â\n",
      "â 21276      â 10462       â 201911  â 0.0     â\n",
      "â 21276      â 10462       â 201912  â 0.0     â\n",
      "ââââââââââââââ´ââââââââââââââ´ââââââââââ´ââââââââââ\n"
     ]
    }
   ],
   "source": [
    "pl.Config.set_tbl_rows(36)\n",
    "print(\"Ejemplo serie con ceros\")\n",
    "print(df_completo.filter((pl.col('product_id')==21276) & (pl.col('customer_id')==10462)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo.write_parquet(f'{folder}/{path_group}')\n",
    "prod_stats.write_parquet(f'{folder}/{path_prod_stats}')\n",
    "overall_prod_stats.write_parquet(f'{folder}/{path_overall_prod_stats}')\n",
    "prod_data.write_parquet(f'{folder}/{path_prod_data}')\n",
    "stock_data.write_parquet(f'{folder}/{path_stock_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------01_LecturaDatos-------------------------------------------\n",
      "----------------------------------------------FINALIZA----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"{fase:-^100}\")\n",
    "print(f\"{'FINALIZA':-^100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_stats = df_grouped_prodcust.group_by(['product_id','customer_id']).agg(\n",
    "#     [\n",
    "#         pl.min('periodo').alias('primer_periodo'),\n",
    "#         pl.max('periodo').alias('ultimo_periodo'),\n",
    "#         pl.len().alias('values'),\n",
    "#         pl.sum('tn').alias('total_tn'),\n",
    "#         pl.min('tn').alias('min_tn'),\n",
    "#         pl.mean('tn').alias('average_tn'),\n",
    "#         pl.median('tn').alias('median_tn'),\n",
    "#         pl.std('tn',0).alias('std_dev_tn'),\n",
    "#         (pl.col('tn').quantile(0.75)).alias('Q3_tn'),\n",
    "#         (pl.col('tn').quantile(0.25)).alias('Q1_tn'),\n",
    "#         pl.max('tn').alias('max_tn')\n",
    "#     ]\n",
    "# ).sort(['product_id', 'customer_id'])\n",
    "\n",
    "# prod_stats = prod_stats.with_columns(\n",
    "#     [\n",
    "#         (pl.col('Q3_tn') - pl.col('Q1_tn')).alias('IQR'),\n",
    "#     ]\n",
    "# ).sort(['product_id', 'customer_id'])\n",
    "\n",
    "# prod_stats = prod_stats.with_columns(\n",
    "#     (pl.col(\"primer_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\"),\n",
    "#     (pl.col(\"ultimo_periodo\").cast(pl.Utf8) + \"01\").str.to_date(\"%Y%m%d\")\n",
    "# )\n",
    "\n",
    "# print(f\"prod_stats Shape:    ({prod_stats.shape[0]:>9_d},{prod_stats.shape[1]:_d}, productos unicos: {prod_stats['product_id'].unique().count()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = df_grouped_prodcust.filter((pl.col('product_id') == 20032) & (pl.col('customer_id') == 10344))\n",
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"\n",
    "#     total = {check['tn'].sum()}\n",
    "#     mean = {check['tn'].mean()}\n",
    "#     median = {check['tn'].median()}\n",
    "#     std = {check['tn'].std(ddof=0)}\n",
    "#     Q3 = {check['tn'].quantile(0.75)}\n",
    "#     Q1 = {check['tn'].quantile(0.25)}\n",
    "#     \"\"\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prod_stats.filter((pl.col('product_id') == 20032) & (pl.col('customer_id') == 10344))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import RobustScaler\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# check = check.with_columns(\n",
    "#     pl.lit(scaler.fit_transform(check['tn'].to_numpy().reshape(-1, 1))).alias('tn_robust_sklearn')\n",
    "#     )\n",
    "# check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# #serie_test = df.filter((pl.col('product_id')==20001) & (pl.col('customer_id')==10001))\n",
    "# serie_test = df.filter((pl.col('product_id')==20003) & (pl.col('customer_id')==10420))\n",
    "# serie_test = serie_test.join(prod_stats[['product_id', 'customer_id', 'average_tn', 'std_dev_tn']], on=['product_id', 'customer_id'], how='left', coalesce=True)\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     pl.lit(scaler.fit_transform(serie_test['tn'].to_numpy().reshape(-1, 1))).alias('tn_standard_sklearn')\n",
    "# ])\n",
    "\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     ((pl.col('tn') - pl.col('average_tn')) / pl.col('std_dev_tn')).alias('tn_standardmanual'),\n",
    "# ])\n",
    "\n",
    "# print(scaler.var_**0.5)\n",
    "\n",
    "# serie_test = serie_test.with_columns(\n",
    "#     pl.col(\"tn_standardmanual\")\n",
    "#     .fill_nan(0)\n",
    "#     .replace([float('inf'), float('-inf')], 0)\n",
    "# )\n",
    "\n",
    "# serie_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import RobustScaler\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# #serie_test = df.filter((pl.col('product_id')==20001) & (pl.col('customer_id')==10001))\n",
    "# serie_test = df.filter((pl.col('product_id')==20003) & (pl.col('customer_id')==10420))\n",
    "# serie_test = serie_test.join(prod_stats[['product_id', 'customer_id', 'median_tn', 'iqr_tn']], on=['product_id', 'customer_id'], how='left', coalesce=True)\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     pl.lit(scaler.fit_transform(serie_test['tn'].to_numpy().reshape(-1, 1))).alias('tn_robust_sklearn')\n",
    "# ])\n",
    "\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     ((pl.col('tn') - pl.col('median_tn')) / pl.col('iqr_tn')).alias('tn_robustmanual'),\n",
    "# ])\n",
    "\n",
    "# print(scaler.scale_)\n",
    "\n",
    "# serie_test = serie_test.with_columns(\n",
    "#     pl.col(\"tn_robustmanual\")\n",
    "#     .fill_nan(0)\n",
    "#     .replace([float('inf'), float('-inf')], 0)\n",
    "# )\n",
    "\n",
    "# serie_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     pl.lit(scaler.fit_transform(serie_test['tn'].to_numpy().reshape(-1, 1))).alias('tn_standard')\n",
    "# ])\n",
    "\n",
    "# serie_test = serie_test.with_columns([\n",
    "#     ((pl.col('tn') - pl.col('average_tn')) / pl.col('std_dev_tn')).alias('tn_standardmanual'),\n",
    "# ])\n",
    "\n",
    "# serie_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
